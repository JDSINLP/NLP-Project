{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30e8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import acquire as a\n",
    "import prepare as p\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbc7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def web_scrape_repos():\n",
    "    \n",
    "#     search_topics = \"https://github.com/search?p=\"\n",
    "\n",
    "#     REPOS = []\n",
    "    \n",
    "#     for page in range(1, 20):\n",
    "\n",
    "#         req = requests.get(search_topics + str(page) + \"&q=\" + 'bitcoin' + \"&type=Repositories\")\n",
    "#         soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "#         repos = soup.find_all('a', class_='v-align-middle')\n",
    "#         for link in repos:\n",
    "#             REPOS.append(link['href'][1:])\n",
    "    \n",
    "#     return REPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f206528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPOS = a.web_scrape_repos()\n",
    "# REPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16a2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dbde9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca79dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#content = p.clean_text(' '.join(df[df['readme_contents']]))\n",
    " \n",
    "#content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b0911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c4cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f402c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c866453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(original):\n",
    "    '''\n",
    "    This function takes in a string and lowers the case, normalizes unicode characters, \n",
    "    and uses regex to replace anything that is not a letter, number, whitespace or a single quote.\n",
    "    '''\n",
    "    # lower it\n",
    "    article = original.lower()\n",
    "    # normalize it\n",
    "    article = unicodedata.normalize('NFKD', article).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # regex it \n",
    "    article = re.sub(r'[^a-z0-9\\s]', ' ', article)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd03e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(article):\n",
    "    '''\n",
    "    This function takes in a string and tokenizes all the words in the string.\n",
    "    '''\n",
    "    # create object\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    # spit out tokenize\n",
    "    string = tokenize.tokenize(article, return_str=True)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca970ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(article):\n",
    "    '''\n",
    "    This function will take in a string and run a PorterStemmer object\n",
    "    - will return a string of all the stems of the words in the article\n",
    "    '''\n",
    "    \n",
    "    # Create the nltk stemmer object, then use it\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    stems = [ps.stem(word) for word in article.split()]\n",
    "    # run stemmer object on article to create stems\n",
    "    \n",
    "    article_stemmed = ' '.join(stems)\n",
    "    # create stemmed article by joining stems\n",
    "    \n",
    "    return article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7546e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(article):\n",
    "    '''\n",
    "    This functions will take in a string and run a WordNetLemmatizer object\n",
    "    - will return a string of lemmatized words from the article\n",
    "    '''\n",
    "    \n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # create lemmatizer object\n",
    "    \n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "    ## run lemmatizer object on article\n",
    "    \n",
    "    article_lemmatized = ' '.join(lemmas)\n",
    "    # create lemmatized article by joining lemmatized words together\n",
    "    \n",
    "    return article_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece8a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(article):\n",
    "    '''\n",
    "    This function will take in a string in the form of an article and remove standard English stop words\n",
    "    - will return a string of remaining words once all desired stop words have been removed \n",
    "    '''\n",
    "    \n",
    "    stopword_list = stopwords.words('english')\n",
    "    # create standard English stop words list\n",
    "    \n",
    "    words = article.split()\n",
    "    # split article into individual words\n",
    "    \n",
    "    filtered_words = [w for w in words if w not in stopword_list]\n",
    "    # filter for words in stop words\n",
    "    \n",
    "    article_without_stopwords = ' '.join(filtered_words)\n",
    "    # recreate article out of remaining words\n",
    "    \n",
    "    return article_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09542436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bitcoin(content):\n",
    "    '''\n",
    "    This function will prepare data from from the df so that it can be used in NLP\n",
    "    models and exploration\n",
    "    - will take in a string a clean it\n",
    "        -lowercase\n",
    "        -remove accented and special characters\n",
    "    - will tokenize the string and return the seperated words\n",
    "    - will lemmatize the content\n",
    "    - will remove standard english stopwords\n",
    "    '''\n",
    "    \n",
    "    # run cleaning function\n",
    "    clean_content = basic_clean(content)\n",
    "    \n",
    "    # run tokenize function\n",
    "    tokenized_content = p.tokenize(clean_content)\n",
    "    \n",
    "    # lemmatize content\n",
    "    lemmatized_content = lemmatize(tokenized_content)\n",
    "    \n",
    "    # remove stopwords\n",
    "    final_content = remove_stopwords(lemmatized_content)\n",
    "    \n",
    "    return final_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6f5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.readme_contents = df.readme_contents.apply(prepare_bitcoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c364a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #chain together clean, tokenize, remove stopwords\n",
    "# df['clean'] = df['readme_contents'].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7f173e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b57ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tokenize'] = df['readme_contents'].apply(tokenize)\n",
    "# df['stem'] = df['readme_contents'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ef9634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "918c3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['no_stopwords'] = df['readme_contents'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df304a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d4462f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_gh_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    #original text from content column\n",
    "    df['original'] = df['readme_contents']\n",
    "    \n",
    "    #chain together clean, tokenize, remove stopwords\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    #chain clean, tokenize, stem, remove stopwords\n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    #clean clean, tokenize, lemmatize, remove stopwords\n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df[['title', 'original', 'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79e09821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep_gh_data(df, df.readme_contents, extra_words=['r', 'u', '2', 'ltgt'], exclude_words=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14574b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.clean_text('readme_contents', extra_stopwords=['r', 'u', '2', 'ltgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b32dd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, row in df.iterrows():\n",
    "    if str(row['language']) in ['Python','JavaScript','C++']:\n",
    "        continue\n",
    "    else:\n",
    "        df.iloc[i]['language'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4971c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bitcoin/bitcoin</td>\n",
       "      <td>C++</td>\n",
       "      <td>Bitcoin Core integration/staging tree\\n=======...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bitcoinbook/bitcoinbook</td>\n",
       "      <td>Other</td>\n",
       "      <td>Code Examples: ![travis_ci](https://travis-ci....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bitcoinj/bitcoinj</td>\n",
       "      <td>Other</td>\n",
       "      <td>image:https://github.com/bitcoinj/bitcoinj/wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bitcoin/bips</td>\n",
       "      <td>Other</td>\n",
       "      <td>People wishing to submit BIPs, first should pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bitcoinjs/bitcoinjs-lib</td>\n",
       "      <td>Other</td>\n",
       "      <td># BitcoinJS (bitcoinjs-lib)\\n[![Github CI](htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Bitcoin-com/paperwallet.bitcoin.com</td>\n",
       "      <td>Other</td>\n",
       "      <td># Bitcoin.com Paper Wallet\\n\\nThe Bitcoin.com ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>DeFiCh/ain</td>\n",
       "      <td>C++</td>\n",
       "      <td>[![Lint](https://github.com/DeFiCh/ain/actions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>rustyrussell/bitcoin-iterate</td>\n",
       "      <td>Other</td>\n",
       "      <td>This is some fast code to iterate over bitcoin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>gcarq/rusty-blockparser</td>\n",
       "      <td>Other</td>\n",
       "      <td># rusty-blockparser\\n\\n[![Build Status](https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>rust-bitcoin/rust-wallet</td>\n",
       "      <td>Other</td>\n",
       "      <td>[![Safety Dance](https://img.shields.io/badge/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    repo language  \\\n",
       "0                        bitcoin/bitcoin      C++   \n",
       "1                bitcoinbook/bitcoinbook    Other   \n",
       "2                      bitcoinj/bitcoinj    Other   \n",
       "3                           bitcoin/bips    Other   \n",
       "4                bitcoinjs/bitcoinjs-lib    Other   \n",
       "..                                   ...      ...   \n",
       "245  Bitcoin-com/paperwallet.bitcoin.com    Other   \n",
       "246                           DeFiCh/ain      C++   \n",
       "247         rustyrussell/bitcoin-iterate    Other   \n",
       "248              gcarq/rusty-blockparser    Other   \n",
       "249             rust-bitcoin/rust-wallet    Other   \n",
       "\n",
       "                                       readme_contents  \n",
       "0    Bitcoin Core integration/staging tree\\n=======...  \n",
       "1    Code Examples: ![travis_ci](https://travis-ci....  \n",
       "2    image:https://github.com/bitcoinj/bitcoinj/wor...  \n",
       "3    People wishing to submit BIPs, first should pr...  \n",
       "4    # BitcoinJS (bitcoinjs-lib)\\n[![Github CI](htt...  \n",
       "..                                                 ...  \n",
       "245  # Bitcoin.com Paper Wallet\\n\\nThe Bitcoin.com ...  \n",
       "246  [![Lint](https://github.com/DeFiCh/ain/actions...  \n",
       "247  This is some fast code to iterate over bitcoin...  \n",
       "248  # rusty-blockparser\\n\\n[![Build Status](https:...  \n",
       "249  [![Safety Dance](https://img.shields.io/badge/...  \n",
       "\n",
       "[250 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce499f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f382877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
